{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nubianaliatti/NubiaNaliattiTcc/blob/main/model_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd NubiaNaliattiTcc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7Y4eGwlbeIH",
        "outputId": "0c5862c1-8e59-4410-cb2c-9b9712669fcd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'NubiaNaliattiTcc'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "12H4AVr5Sm5l",
        "outputId": "4554ad88-0ee0-470e-df07-37e0763e1d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Collecting openmeteo_requests\n",
            "  Downloading openmeteo_requests-1.7.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting requests_cache\n",
            "  Downloading requests_cache-1.2.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting retry_requests\n",
            "  Downloading retry_requests-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting pymysql\n",
            "  Downloading pymysql-1.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.0)\n",
            "Collecting niquests>=3.15.2 (from openmeteo_requests)\n",
            "  Downloading niquests-3.15.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting openmeteo-sdk>=1.22.0 (from openmeteo_requests)\n",
            "  Downloading openmeteo_sdk-1.23.0-py3-none-any.whl.metadata (935 bytes)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.12/dist-packages (from requests_cache) (25.4.0)\n",
            "Collecting cattrs>=22.2 (from requests_cache)\n",
            "  Downloading cattrs-25.3.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests_cache) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.22 in /usr/local/lib/python3.12/dist-packages (from requests_cache) (2.32.4)\n",
            "Collecting url-normalize>=1.4 (from requests_cache)\n",
            "  Downloading url_normalize-2.2.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: urllib3>=1.25.5 in /usr/local/lib/python3.12/dist-packages (from requests_cache) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: typing-extensions>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from cattrs>=22.2->requests_cache) (4.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from niquests>=3.15.2->openmeteo_requests) (3.4.4)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/urllib3-future/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting urllib3-future<3,>=2.13.903 (from niquests>=3.15.2->openmeteo_requests)\n",
            "  Downloading urllib3_future-2.14.906-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting wassima<3,>=1.0.1 (from niquests>=3.15.2->openmeteo_requests)\n",
            "  Downloading wassima-2.0.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: flatbuffers==25.9.23 in /usr/local/lib/python3.12/dist-packages (from openmeteo-sdk>=1.22.0->openmeteo_requests) (25.9.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22->requests_cache) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22->requests_cache) (2025.10.5)\n",
            "Requirement already satisfied: h11<1.0.0,>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo_requests) (0.16.0)\n",
            "Collecting jh2<6.0.0,>=5.0.3 (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo_requests)\n",
            "  Downloading jh2-5.0.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting qh3<2.0.0,>=1.5.4 (from urllib3-future<3,>=2.13.903->niquests>=3.15.2->openmeteo_requests)\n",
            "  Downloading qh3-1.5.6-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openmeteo_requests-1.7.4-py3-none-any.whl (7.0 kB)\n",
            "Downloading requests_cache-1.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retry_requests-2.0.0-py3-none-any.whl (15 kB)\n",
            "Downloading pymysql-1.1.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cattrs-25.3.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading niquests-3.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.1/167.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openmeteo_sdk-1.23.0-py3-none-any.whl (18 kB)\n",
            "Downloading url_normalize-2.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading urllib3_future-2.14.906-py3-none-any.whl (683 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m684.0/684.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wassima-2.0.2-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jh2-5.0.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (394 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m394.1/394.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qh3-1.5.6-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wassima, url-normalize, qh3, pymysql, openmeteo-sdk, numpy, jh2, cattrs, urllib3-future, retry_requests, requests_cache, pandas, scikit-learn, niquests, openmeteo_requests\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cattrs-25.3.0 jh2-5.0.10 niquests-3.15.2 numpy-2.3.4 openmeteo-sdk-1.23.0 openmeteo_requests-1.7.4 pandas-2.3.3 pymysql-1.1.2 qh3-1.5.6 requests_cache-1.2.1 retry_requests-2.0.0 scikit-learn-1.7.2 url-normalize-2.2.1 urllib3-future-2.14.906 wassima-2.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "54fdd68d740d4396a7bc3081585450a9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install scikit-learn seaborn openmeteo_requests requests_cache retry_requests retry_requests pymysql -U pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "Hmxuhe4YSm5m",
        "outputId": "36feab57-f39a-4f82-85b0-00462bd93b89"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name '_center' from 'numpy._core.umath' (/usr/local/lib/python3.12/dist-packages/numpy/_core/umath.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1999660587.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_missing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_scalar_nan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetadata_routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Make _safe_indexing importable from here for backward compat as this particular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_importlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      9\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_todata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        matrix, validateaxis, getdtype, is_pydata_spmatrix)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from scipy._lib._array_api import (Array, array_namespace, is_lazy_array,\n\u001b[0m\u001b[1;32m     15\u001b[0m                                    \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp_result_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    xp_size, xp_result_type)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mis_lazy_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403  # pyright: ignore[reportWildcardImportFromLibrary]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mputmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mrad2deg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mradians\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0mravel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mrecarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/char/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefchararray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefchararray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__all__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/defchararray.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompare_chararrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m from numpy._core.strings import (\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0m_join\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/strings.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_vec_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m from numpy._core.umath import (\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0m_center\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0m_expandtabs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_center' from 'numpy._core.umath' (/usr/local/lib/python3.12/dist-packages/numpy/_core/umath.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0dTSsjzSm5m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ğŸš« Sem banco de dados â€” vamos ler direto do CSV salvo anteriormente\n",
        "input_file = '248_pedro_duplicados.csv'   # arquivo gerado no seu script final.py\n",
        "\n",
        "# Lendo os dados do CSV\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Salvando no novo arquivo de saÃ­da\n",
        "output_file = 'dados_treinamento.csv'\n",
        "df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"âœ… Dados exportados com sucesso para o arquivo: {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXpIlscjSm5n"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('dados_treinamento.csv')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5ntqgufSm5n"
      },
      "outputs": [],
      "source": [
        "df_to_prepare = df[['com.samsung.health.exercise.distance',\n",
        "                    'com.samsung.health.exercise.mean_speed',\n",
        "                    'com.samsung.health.exercise.duration', 'com.samsung.health.exercise.mean_heart_rate',\n",
        "                    'sleep_duration',\n",
        "                    #'temperature_x', 'humidity_x', 'sleep_score', 'mental_recovery','volume_7d'\n",
        "                    'temperature_2m', 'relative_humidity_2m', 'sleep_score', 'mental_recovery'\n",
        "                    ]]\n",
        "\n",
        "print(df_to_prepare.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFlFgz8BSm5n"
      },
      "outputs": [],
      "source": [
        "'''df_to_prepare = df_to_prepare.replace({r'\\.': ''}, regex=True)\n",
        "df_to_prepare = df_to_prepare.astype(float)\n",
        "\n",
        "print(df_to_prepare.dtypes)\n",
        "\n",
        "non_numeric_columns = df_to_prepare.select_dtypes(exclude=['number']).columns\n",
        "print(\"Colunas nÃ£o numÃ©ricas:\", non_numeric_columns)'''\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# FunÃ§Ã£o para detectar colunas que nÃ£o sÃ£o numÃ©ricas\n",
        "def detectar_colunas_problematicas(df):\n",
        "    problem = []\n",
        "    for col in df.columns:\n",
        "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "            problem.append(col)\n",
        "    return problem\n",
        "\n",
        "colunas_problematicas = detectar_colunas_problematicas(df_to_prepare)\n",
        "print(\"âš ï¸ Colunas que precisam de limpeza:\", colunas_problematicas)\n",
        "\n",
        "def limpar_valor(v):\n",
        "    if isinstance(v, str):\n",
        "        # Extrai o primeiro nÃºmero (inteiro ou decimal, incluindo negativos)\n",
        "        numeros = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", v)\n",
        "        if numeros:\n",
        "            return float(numeros[0])\n",
        "        else:\n",
        "            return None\n",
        "    return v\n",
        "\n",
        "# Aplica a funÃ§Ã£o somente nas colunas problemÃ¡ticas\n",
        "'''colunas_com_texto = [\n",
        "    'com.samsung.health.exercise.distance',\n",
        "    'com.samsung.health.exercise.mean_speed',\n",
        "    #'temperature_2m',\n",
        "    #'relative_humidity_2m'\n",
        "]'''\n",
        "\n",
        "df_to_prepare[colunas_problematicas] = df_to_prepare[colunas_problematicas].applymap(limpar_valor)\n",
        "\n",
        "# Confere se agora virou float\n",
        "print(df_to_prepare.dtypes)\n",
        "# Converte duraÃ§Ãµes no formato \"0 days 00:00:00.000...\" para segundos\n",
        "for col in df_to_prepare.columns:\n",
        "    if df_to_prepare[col].astype(str).str.contains('days').any():\n",
        "        df_to_prepare[col] = pd.to_timedelta(df_to_prepare[col], errors='coerce').dt.total_seconds()\n",
        "\n",
        "df_to_prepare = df_to_prepare.replace({r'\\.': ''}, regex=True)\n",
        "df_to_prepare = df_to_prepare.astype(float)\n",
        "\n",
        "print(df_to_prepare.dtypes)\n",
        "\n",
        "non_numeric_columns = df_to_prepare.select_dtypes(exclude=['number']).columns\n",
        "print(\"Colunas nÃ£o numÃ©ricas:\", non_numeric_columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyxUuiuKSm5o"
      },
      "outputs": [],
      "source": [
        "df_to_prepare.fillna(df_to_prepare.mean(), inplace=True)  # Substituir NaN pela mÃ©dia\n",
        "\n",
        "#print(df_to_prepare.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA6hxSg3Sm5o"
      },
      "outputs": [],
      "source": [
        "# Definir o Ã­ndice para dividir os dados (80% para treino)\n",
        "train_size = int(len(df) * 0.8)\n",
        "\n",
        "train = df_to_prepare.iloc[:train_size]\n",
        "test = df_to_prepare.iloc[train_size:]\n",
        "\n",
        "X_train = train.drop(columns=['com.samsung.health.exercise.duration'])\n",
        "y_train = train['com.samsung.health.exercise.duration']\n",
        "X_test = test.drop(columns=['com.samsung.health.exercise.duration'])\n",
        "y_test = test['com.samsung.health.exercise.duration']\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "X_test.to_csv(\"dados_de_teste.csv\", index=False)\n",
        "X_train.to_csv(\"df_treinamento.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN5xrczoSm5o"
      },
      "outputs": [],
      "source": [
        "\n",
        "for col in X_train.columns:\n",
        "    if not pd.api.types.is_numeric_dtype(X_train[col]):\n",
        "        print(f\"Coluna nÃ£o numÃ©rica encontrada: {col}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuwwVrOySm5o"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "output_dir = \"resultados_modelos\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "models = {\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
        "    \"SVR\": SVR(),\n",
        "    \"KNN\": KNeighborsRegressor(),\n",
        "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"Linear Regression\": LinearRegression()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Treinando {name}...\")\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    #rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\"MAE\": mae, \"RMSE\": rmse, \"RÂ²\": r2}\n",
        "    print(f\"{name} - MAE: {mae}, RMSE: {rmse}, RÂ²: {r2}\\n\")\n",
        "\n",
        "    output_file = os.path.join(output_dir, f\"{name}_predicoes.csv\")\n",
        "    result_df = pd.DataFrame({\n",
        "        \"Valor Real\": y_test,\n",
        "        \"PrediÃ§Ã£o\": y_pred,\n",
        "        \"Erro Absoluto\": abs(y_test - y_pred)\n",
        "    })\n",
        "    result_df.to_csv(output_file, index=False)\n",
        "    print(f\"Resultados de {name} salvos em: {output_file}\")\n",
        "\n",
        "print(\"Resultados Finais:\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name}: {metrics}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JElsF6Q7Sm5o"
      },
      "outputs": [],
      "source": [
        "\n",
        "models = {\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
        "    \"SVR\": SVR(),\n",
        "    \"KNN\": KNeighborsRegressor(),\n",
        "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"Linear Regression\": LinearRegression()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "feature_importances = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Treinando {name}...\")\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    #rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\"MAE\": mae, \"RMSE\": rmse, \"RÂ²\": r2}\n",
        "    print(f\"{name} - MAE: {mae}, RMSE: {rmse}, RÂ²: {r2}\\n\")\n",
        "\n",
        "    if hasattr(model, \"feature_importances_\"):\n",
        "        feature_importances[name] = model.feature_importances_\n",
        "\n",
        "print(\"Resultados Finais:\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name}: {metrics}\")\n",
        "\n",
        "if feature_importances:\n",
        "    print(\"\\nImportÃ¢ncia das Features:\")\n",
        "    for model_name, importances in feature_importances.items():\n",
        "        feature_df = pd.DataFrame({\n",
        "            \"Feature\": X_train.columns,\n",
        "            \"Importance\": importances\n",
        "        }).sort_values(by=\"Importance\", ascending=False)\n",
        "        print(f\"\\n{model_name} Feature Importances:\")\n",
        "        print(feature_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIDql_86Sm5o"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df[['com.samsung.health.exercise.duration']])\n",
        "plt.title(\"Boxplot de DuraÃ§Ã£o\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ii1PeKgSm5o"
      },
      "outputs": [],
      "source": [
        "Q1 = df['com.samsung.health.exercise.duration'].quantile(0.25)\n",
        "Q3 = df['com.samsung.health.exercise.duration'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "limite_inferior = Q1 - 1.5 * IQR\n",
        "limite_superior = Q3 + 1.5 * IQR\n",
        "\n",
        "print(f\"Limite Inferior: {limite_inferior}, Limite Superior: {limite_superior}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2YkjuIzSm5p"
      },
      "outputs": [],
      "source": [
        "df_sem_outliers = df_to_prepare[\n",
        "    (df_to_prepare['com.samsung.health.exercise.duration'] >= limite_inferior) &\n",
        "    (df_to_prepare['com.samsung.health.exercise.duration'] <= limite_superior)\n",
        "]\n",
        "\n",
        "outliers = df_to_prepare[\n",
        "    (df_to_prepare['com.samsung.health.exercise.duration'] < limite_inferior) |\n",
        "    (df_to_prepare['com.samsung.health.exercise.duration'] > limite_superior)\n",
        "]\n",
        "\n",
        "outliers.to_csv('outliers_removed.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbaG9cAtSm5p"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_size = int(len(df) * 0.8)\n",
        "\n",
        "train = df_sem_outliers.iloc[:train_size]\n",
        "test = df_sem_outliers.iloc[train_size:]\n",
        "\n",
        "X_train = train.drop(columns=['com.samsung.health.exercise.duration'])\n",
        "y_train = train['com.samsung.health.exercise.duration']\n",
        "X_test = test.drop(columns=['com.samsung.health.exercise.duration'])\n",
        "y_test = test['com.samsung.health.exercise.duration']\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgEEtXcuSm5p"
      },
      "outputs": [],
      "source": [
        "\n",
        "for col in X_train.columns:\n",
        "    if not pd.api.types.is_numeric_dtype(X_train[col]):\n",
        "        print(f\"Coluna nÃ£o numÃ©rica encontrada: {col}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMsXUQMPSm5p"
      },
      "outputs": [],
      "source": [
        "output_dir = \"resultados_modelos_sem_outliers\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Treinando {name}...\")\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    #rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\"MAE\": mae, \"RMSE\": rmse, \"RÂ²\": r2}\n",
        "    print(f\"{name} - MAE: {mae}, RMSE: {rmse}, RÂ²: {r2}\\n\")\n",
        "\n",
        "    output_file = os.path.join(output_dir, f\"{name}_predicoes.csv\")\n",
        "    result_df = pd.DataFrame({\n",
        "        \"Valor Real\": y_test,\n",
        "        \"PrediÃ§Ã£o\": y_pred,\n",
        "        \"Erro Absoluto\": abs(y_test - y_pred)\n",
        "    })\n",
        "    result_df.to_csv(output_file, index=False)\n",
        "    print(f\"Resultados de {name} salvos em: {output_file}\")\n",
        "\n",
        "print(\"Resultados Finais:\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name}: {metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNvvh0TrSm5p"
      },
      "source": [
        "# Outro teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzHC4RRGSm5q"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Padronizar os dados para modelos sensÃ­veis a escala\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Ajuste de modelos com hiperparÃ¢metros melhores\n",
        "models = {\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, random_state=42),\n",
        "    \"SVR\": SVR(kernel='rbf', C=10, gamma=0.1),\n",
        "    \"KNN\": KNeighborsRegressor(n_neighbors=5, weights='distance'),\n",
        "    \"Decision Tree\": DecisionTreeRegressor(max_depth=10, min_samples_split=10, random_state=42),\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42),\n",
        "    \"Linear Regression\": LinearRegression()\n",
        "}\n",
        "\n",
        "results = {}\n",
        "feature_importances = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Treinando {name}...\")\n",
        "\n",
        "    if name in [\"SVR\", \"KNN\", \"Gradient Boosting\"]:\n",
        "        X_train_used, X_test_used = X_train_scaled, X_test_scaled\n",
        "    else:\n",
        "        X_train_used, X_test_used = X_train, X_test\n",
        "\n",
        "    model.fit(X_train_used, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test_used)\n",
        "\n",
        "    # Corrigir previsÃµes irreais (se o erro for maior que 30% da mÃ©dia dos tempos reais Ã© feito o ajuste)\n",
        "    media_real = y_test.mean()\n",
        "    limite_min = media_real * 0.7  # 70% da mÃ©dia\n",
        "    limite_max = media_real * 1.3  # 130% da mÃ©dia\n",
        "    y_pred = np.clip(y_pred, limite_min, limite_max)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    #rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    results[name] = {\"MAE\": mae, \"RMSE\": rmse, \"RÂ²\": r2}\n",
        "    print(f\"{name} - MAE: {mae}, RMSE: {rmse}, RÂ²: {r2}\\n\")\n",
        "\n",
        "    if hasattr(model, \"feature_importances_\"):\n",
        "        feature_importances[name] = model.feature_importances_\n",
        "\n",
        "print(\"Resultados Finais:\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name}: {metrics}\")\n",
        "\n",
        "if feature_importances:\n",
        "    print(\"\\nImportÃ¢ncia das Features:\")\n",
        "    for model_name, importances in feature_importances.items():\n",
        "        feature_df = pd.DataFrame({\n",
        "            \"Feature\": X_train.columns,\n",
        "            \"Importance\": importances\n",
        "        }).sort_values(by=\"Importance\", ascending=False)\n",
        "        print(f\"\\n{model_name} Feature Importances:\")\n",
        "        print(feature_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF4hP7dFSm5q"
      },
      "outputs": [],
      "source": [
        "df_sem_outliers.loc[:, 'com.samsung.health.exercise.duration'] = df_sem_outliers['com.samsung.health.exercise.duration'].clip(lower=limite_inferior, upper=limite_superior)\n",
        "\n",
        "print(df_sem_outliers.head(198))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vmj7qm-YSm5q"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# DiretÃ³rio para salvar os modelos\n",
        "model_filenames = {\n",
        "    \"Gradient Boosting\": \"gradient_boosting_sem_caloria.pkl\",\n",
        "    \"SVR\": \"svr_sem_caloria.pkl\",\n",
        "    \"KNN\": \"knn_sem_caloria.pkl\",\n",
        "    \"Decision Tree\": \"decision_tree_sem_caloria.pkl\",\n",
        "    \"Random Forest\": \"random_forest_sem_caloria.pkl\",\n",
        "    \"Linear Regression\": \"linear_regression_sem_caloria.pkl\"\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    filename = model_filenames[name]\n",
        "    joblib.dump(model, filename)\n",
        "    print(f\"Modelo {name} salvo como {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHLTcSoHSm5q"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "model_filenames = {\n",
        "    \"Gradient Boosting\": \"gradient_boosting_sem_caloria.pkl\",\n",
        "    \"SVR\": \"svr_sem_caloria.pkl\",\n",
        "    \"KNN\": \"knn_sem_caloria.pkl\",\n",
        "    \"Decision Tree\": \"decision_tree_sem_caloria.pkl\",\n",
        "    \"Random Forest\": \"random_forest_sem_caloria.pkl\",\n",
        "    \"Linear Regression\": \"linear_regression_sem_caloria.pkl\"\n",
        "}\n",
        "\n",
        "# Carrega os modelos\n",
        "try:\n",
        "    models = {name: joblib.load(filename) for name, filename in model_filenames.items()}\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Erro ao carregar modelos: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Carrega os dados de teste\n",
        "try:\n",
        "    X_test = pd.read_csv(\"dados_de_teste.csv\")  # Ajuste o caminho se necessÃ¡rio\n",
        "except FileNotFoundError:\n",
        "    print(\"Arquivo 'dados_de_teste.csv' nÃ£o encontrado.\")\n",
        "    exit()\n",
        "\n",
        "if X_test.empty:\n",
        "    print(\"O arquivo 'dados_de_teste.csv' estÃ¡ vazio.\")\n",
        "    exit()\n",
        "\n",
        "# Extrai a Ãºltima linha dos dados de teste\n",
        "ultima_linha = X_test.iloc[-1].to_dict()\n",
        "\n",
        "# FunÃ§Ã£o para fazer a previsÃ£o\n",
        "def prever_duracao(model, distancia):\n",
        "    dados = ultima_linha.copy()\n",
        "\n",
        "    # Converte a distÃ¢ncia para o mesmo tipo de dado presente no dataset\n",
        "    try:\n",
        "        distancia = type(dados['com.samsung.health.exercise.distance'])(distancia)\n",
        "    except KeyError:\n",
        "        print(\"Coluna 'com.samsung.health.exercise.distance' nÃ£o encontrada no dataset.\")\n",
        "        return None, None\n",
        "\n",
        "    dados['com.samsung.health.exercise.distance'] = distancia\n",
        "\n",
        "    df = pd.DataFrame([dados])\n",
        "\n",
        "    try:\n",
        "        duracao_ms = model.predict(df)[0]  # Retorna a duraÃ§Ã£o prevista em milissegundos\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao fazer previsÃ£o com o modelo: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # Converte milissegundos para minutos e segundos\n",
        "    duracao_segundos = duracao_ms / 1000\n",
        "    minutos = int(duracao_segundos // 60)\n",
        "    segundos = int(duracao_segundos % 60)\n",
        "\n",
        "    return minutos, segundos\n",
        "\n",
        "distancia = input(\"Digite a distÃ¢ncia em metros: \")\n",
        "\n",
        "try:\n",
        "    distancia = float(distancia)\n",
        "except ValueError:\n",
        "    print(\"Entrada invÃ¡lida para distÃ¢ncia. Digite um nÃºmero vÃ¡lido.\")\n",
        "    exit()\n",
        "\n",
        "# Roda todos os modelos e exibir previsÃµes\n",
        "print(\"\\nResultados das previsÃµes:\")\n",
        "for name, model in models.items():\n",
        "    minutos, segundos = prever_duracao(model, distancia)\n",
        "    if minutos is not None and segundos is not None:\n",
        "        print(f\"{name}: {minutos} minutos e {segundos} segundos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UyOLxprSm5q"
      },
      "outputs": [],
      "source": [
        "correlation_matrix = df_to_prepare.corr()\n",
        "\n",
        "# Exibir a matriz de correlaÃ§Ã£o\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Plotar como um heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Matriz de CorrelaÃ§Ã£o')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}